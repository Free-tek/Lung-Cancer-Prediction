{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f361cb90ba5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDBatching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationCost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bgd'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bgd.batch import SGDBatching\n",
    "from bgd.cost import MSE, CrossEntropy, ClassificationCost\n",
    "from bgd.layers import *\n",
    "from bgd.nn import NeuralStack\n",
    "from bgd.optimizers import MomentumOptimizer, AdamOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp_on_xor_problem():\n",
    "    nn = NeuralStack()\n",
    "    nn.add(FullyConnected(2, 8))\n",
    "    nn.add(Activation(function='tanh'))\n",
    "    nn.add(FullyConnected(8, 1))\n",
    "    nn.add(Activation(function='sigmoid'))\n",
    "    nn.add(MSE())\n",
    "    nn.add(MomentumOptimizer(learning_rate=.5, momentum=0))\n",
    "    nn.add(SGDBatching(4))\n",
    "    X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "    y = [0, 1, 1, 0]\n",
    "    nn.train(X, y, l2_alpha=0, epochs=1000, print_every=10)\n",
    "    predictions = np.squeeze(nn.eval(X))\n",
    "    assert np.array_equal(predictions > 0.5, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_learnable():\n",
    "    non_learnable_layers = list()\n",
    "    non_learnable_layers.append(Activation(function='tanh'))\n",
    "    non_learnable_layers.append(Flatten())\n",
    "    non_learnable_layers.append(Dropout())\n",
    "    non_learnable_layers.append(GaussianNoise(0, 1))\n",
    "    non_learnable_layers.append(MaxPooling2D((2, 2)))\n",
    "    non_learnable_layers.append(Lambda(lambda x: 2*x, lambda x: 0.5*x))\n",
    "    for layer in non_learnable_layers:\n",
    "        assert not layer.learnable()\n",
    "    learnable_layers = list()\n",
    "    learnable_layers.append(Convolutional2D((3, 3, 4), 16))\n",
    "    learnable_layers.append(FullyConnected(50, 70))\n",
    "    for layer in learnable_layers:\n",
    "        assert layer.learnable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp_digits():\n",
    "    images, labels = load_digits(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, train_size=.9, test_size=.1)\n",
    "    nn = NeuralStack()\n",
    "    nn.add(FullyConnected(64, 32))\n",
    "    nn.add(Activation('relu'))\n",
    "    nn.add(FullyConnected(32, 10))\n",
    "    nn.add(Activation('softmax'))\n",
    "    nn.add(SGDBatching(len(X_train)))\n",
    "    nn.add(CrossEntropy())\n",
    "    nn.add(AdamOptimizer(learning_rate=5e-3))\n",
    "    nn.train(X_train, y_train, print_every=1, epochs=100, l2_alpha=.05)\n",
    "    assert ClassificationCost.accuracy(y_test, nn.eval(X_test)) >= .9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bounds_activations():\n",
    "    bounds = {\n",
    "        'softmax': (0, 1),\n",
    "        'sigmoid': (0, 1),\n",
    "        'tanh': (-1, +1)\n",
    "    }\n",
    "    X = np.random.normal(loc=0, scale=1, size=(1000, 1000))\n",
    "    for activation in bounds:\n",
    "        activation_layer = Activation(activation)\n",
    "        output = activation_layer.forward(X)\n",
    "        m, M = bounds[activation]\n",
    "        assert not np.logical_or(output < m, output > M).any()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu():\n",
    "    X = np.array(\n",
    "        [[-1, 5, 2, 4, -1],\n",
    "         [2, -2, 3, -1, 0],\n",
    "         [-1, -2, -3, -4, 0]\n",
    "        ]\n",
    "    )\n",
    "    layer = Activation('relu')\n",
    "    expected_output_forward = np.array(\n",
    "        [[0, 5, 2, 4, 0],\n",
    "         [2, 0, 3, 0, 0],\n",
    "         [0, 0, 0, 0, 0]\n",
    "        ]\n",
    "    )\n",
    "    assert (layer.forward(X) == expected_output_forward).all()\n",
    "    error = np.ones((3, 5))\n",
    "    expected_output_backward = np.array(\n",
    "        [[0, 1, 1, 1, 0],\n",
    "         [1, 0, 1, 0, 0],\n",
    "         [0, 0, 0, 0, 0]\n",
    "        ]\n",
    "    )\n",
    "    assert (layer.backward(error)[0] == expected_output_backward).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn_digits():\n",
    "    images, labels = load_digits(return_X_y=True)\n",
    "    images = images.reshape((-1, 8, 8))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, train_size=.9, test_size=.1)\n",
    "    nn = NeuralStack()\n",
    "    nn.add(Convolutional2D((4, 4, 1), 16))\n",
    "    nn.add(Activation('relu'))\n",
    "    nn.add(MaxPooling2D((2, 2)))\n",
    "    nn.add(Flatten())\n",
    "    nn.add(FullyConnected(256, 64))\n",
    "    nn.add(Activation('sigmoid'))\n",
    "    nn.add(FullyConnected(64, 10))\n",
    "    nn.add(Activation('softmax'))\n",
    "    nn.add(SGDBatching(len(X_train)))\n",
    "    nn.add(CrossEntropy())\n",
    "    nn.add(AdamOptimizer(learning_rate=5e-3))\n",
    "    nn.train(X_train, y_train, print_every=5, epochs=150, l2_alpha=.05, validation_fraction=0)\n",
    "    print('Trained')\n",
    "    assert ClassificationCost.accuracy(y_test, nn.eval(X_test)) >= .9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
